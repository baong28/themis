import json
import os, re, sys
import io
import pickle
import faiss
import fitz  # PyMuPDF
import dropbox
import pytesseract
from PyPDF2 import PdfReader
import torch
from PIL import Image
from pdf2image import convert_from_bytes
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer
import streamlit as st
import pytesseract
import sqlite3
# N·∫øu Windows, set ƒë∆∞·ªùng d·∫´n c·ª• th·ªÉ n·∫øu kh√¥ng trong PATH
if sys.platform.startswith("win"):
    pytesseract.pytesseract.tesseract_cmd = r"C:\Users\baong\AppData\Local\Programs\Tesseract-OCR\tesseract.exe"

# ==============================
# üîß 1. CONFIGURATION
# ==============================
CONFIG_FILE = "config.json"
def load_local_config():
    """Load Dropbox credentials from local JSON file."""
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r") as f:
            data = json.load(f)
            return data
    else:
        return {}

local_cfg = load_local_config()
APP_KEY = st.secrets["dropbox"]["app_key"]
APP_SECRET = st.secrets["dropbox"]["app_secret"]
ACCESS_TOKEN = st.secrets["dropbox"]["access_token"]
REFRESH_TOKEN = st.secrets["dropbox"]["refresh_token"] or local_cfg.get("refresh_token")
OPENAI_API_KEY = st.secrets["openai"]["api_key"]

FOLDER_PATH = "/Apps/Document Brain"  
FAISS_DIR = "data/faiss_store"
INDEX_PATH = os.path.join(FAISS_DIR, "index.faiss")
META_PATH = os.path.join(FAISS_DIR, "metadata.pkl")
SQLITE_DB_PATH = os.path.join(FAISS_DIR, "metadata.db")
embedding_model = "text-embedding-3-large"
SENTENCE_TRANSFORMER_NAME = "BAAI/bge-large-en-v1.5"  # or bge-small if constrained

CHUNK_SIZE = 800 # 2000
CHUNK_OVERLAP = 120 # 150

# ========== MODEL LOADING (cached for streamlit) ==========
@st.cache_resource(show_spinner=False)
def load_embedding_model(model_name=SENTENCE_TRANSFORMER_NAME):
    return SentenceTransformer(model_name)

def get_dropbox_client():
    """
    Returns an authenticated Dropbox client.
    If no refresh token exists, runs OAuth flow to get one.
    """
    dbx = dropbox.Dropbox(
        oauth2_refresh_token=REFRESH_TOKEN,
        app_key=APP_KEY,
        app_secret=APP_SECRET,
    )

    # Otherwise, run OAuth flow to get new refresh token
    print("‚öôÔ∏è No refresh token found. Starting Dropbox OAuth flow...")
    auth_flow = dropbox.DropboxOAuth2FlowNoRedirect(
        consumer_key=APP_KEY,
        consumer_secret=APP_SECRET,     
        token_access_type="offline"    
    )

    authorize_url = auth_flow.start()
    print("1Ô∏è‚É£ Go to this URL in your browser:")
    print(authorize_url)
    print("2Ô∏è‚É£ Click 'Allow' and copy the authorization code.")
    auth_code = input("3Ô∏è‚É£ Enter the code here: ").strip()

    oauth_result = auth_flow.finish(auth_code)

    # Save tokens locally
    config_data = {
        "refresh_token": oauth_result.refresh_token,
        "account_id": oauth_result.account_id,
    }
    with open(CONFIG_FILE, "w") as f:
        json.dump(config_data, f, indent=2)

    print(f"‚úÖ Refresh token saved to {CONFIG_FILE}")
    dbx = dropbox.Dropbox(
        oauth2_refresh_token=oauth_result.refresh_token,
        app_key=APP_KEY,
        app_secret=APP_SECRET,
    )
    return dbx

def init_sqlite():
    """
        Create table to store metadata if it does not exist.

    """
    os.makedirs(FAISS_DIR, exist_ok=True)
    conn = sqlite3.connect(SQLITE_DB_PATH)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS chunks (
        id TEXT PRIMARY KEY,
        filename TEXT,
        path TEXT,
        page INTEGER,
        chunk_index INTEGER,
        chunk_chars INTEGER,
        has_ocr INTEGER,
        collection_id TEXT,
        content TEXT
    )
    """)
    conn.commit()
    conn.close()

def insert_metadata(docs):
    """
        Insert new metadata into SQLite, skipping existing chunks.
    """
    conn = sqlite3.connect(SQLITE_DB_PATH)
    cur = conn.cursor()
    existing_ids = set(r[0] for r in cur.execute("SELECT id FROM chunks").fetchall())

    new_rows = []
    for d in docs:
        meta = d["metadata"]
        if meta["bates_id"] in existing_ids:
            continue
        new_rows.append((
            meta["bates_id"],
            meta["source"],
            meta["path"],
            meta["page"],
            meta["chunk_index"],
            meta["chunk_chars"],
            int(meta["has_ocr"]),
            meta["collection_id"],
            d["content"]
        ))

    cur.executemany("""
        INSERT INTO chunks (
            id, filename, path, page, chunk_index, chunk_chars, has_ocr, collection_id, content
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, new_rows)
    conn.commit()
    conn.close()
    print(f"üíæ Saved {len(new_rows)} metadata entries to SQLite.")

def clean_page_text(text: str) -> str:
    """
    L√†m s·∫°ch n·ªôi dung trang PDF, lo·∫°i b·ªè header/footer, s·ªë trang, watermark, k√Ω t·ª± nhi·ªÖu.
    √Åp d·ª•ng ƒë∆∞·ª£c cho ƒëa d·∫°ng lo·∫°i t√†i li·ªáu (academic, legal, technical, OCR, v.v.)
    """
    if not text:
        return ""
    
    # Chu·∫©n h√≥a k√Ω t·ª± tr·∫Øng & xu·ªëng d√≤ng
    text = text.replace('\xa0', ' ').replace('\t', ' ')
    text = re.sub(r'\s+', ' ', text).strip()

    # Preserve line breaks to detect header/footer lines, then normalize
    lines = [ln.strip() for ln in text.splitlines()]
    cleaned_lines = []

    for line in lines:
        l = line.strip()
        if not l or len(l) < 3:
            continue
        # -----------------------------
        # üîπ Lo·∫°i b·ªè header/footer ph·ªï bi·∫øn
        # -----------------------------
        if re.match(r'^(page|p\.)\s*\d+(\s*of\s*\d+)?$', l, re.I): continue
        if re.match(r'^\d+\s*/\s*\d+$', l): continue
        if re.match(r'^\d{1,3}$', l): continue
        if re.search(r'\bdoi\.org/\S+', l, re.I): continue
        if re.search(r'\bISSN\b|\bISBN\b|\bjournal\b|\bmanuscript\b', l, re.I): continue
        if re.search(r'¬©\s*\d{4}', l) or re.search(r'copyright', l, re.I): continue
        if re.search(r'www\.|http[s]?://', l, re.I): continue
        if re.search(r'(university|faculty|institute|department|school of)', l, re.I): continue
        if re.search(r'(int\.|journal|conference|proceedings|res\.)', l, re.I):
            if len(l.split()) < 10: continue
        if re.search(r'(exhibit|deposition|confidential|attorneys eyes only)', l, re.I): continue
        if re.search(r'(Bates\s*(No|Number|ID)?\s*[:#]?)', l, re.I): continue
        if re.search(r'(draft|internal use only|company confidential)', l, re.I): continue
        if re.search(r'(page \d+)|(continued on next page)', l, re.I): continue
        if re.match(r'^[A-Za-z]$', l): continue  # ch·ªâ 1 ch·ªØ c√°i l·∫ª
        # if re.search(r'[\u25A0-\u25FF\u2022\u00B7]', l): l = re.sub(r'[\u25A0-\u25FF\u2022\u00B7]', '', l)

        # remove bullet glyphs
        l = re.sub(r'[\u2022\u00B7\u25A0-\u25FF]', '', l)
        # drop lines with only punctuation
        if re.match(r'^[^\w\s]{3,}$', l):
            continue

        cleaned_lines.append(l)

    # -----------------------------
    # üîπ H·∫≠u x·ª≠ l√Ω
    # -----------------------------
    cleaned_text = " ".join(cleaned_lines)

    # X√≥a kho·∫£ng tr·∫Øng d∆∞ th·ª´a, d·∫•u l·∫∑p
    cleaned_text = re.sub(r'\s{2,}', ' ', cleaned_text)
    cleaned_text = re.sub(r'-\s+', '', cleaned_text)  # n·ªëi c√°c t·ª´ b·ªã ng·∫Øt d√≤ng
    cleaned_text = cleaned_text.strip()

    return cleaned_text

# ========== SMART CHUNKER (sentence-accumulation) ==========
_SENTENCE_SPLIT_RE = re.compile(r'(?<=[\.\?\!\n])\s+')
def smart_chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    if not text:
        return []
    sentences = _SENTENCE_SPLIT_RE.split(text)
    chunks = []
    cur = ""
    for s in sentences:
        if len(cur) + len(s) <= chunk_size:
            if cur:
                cur += " " + s
            else:
                cur = s
        else:
            # finalize current chunk
            if cur:
                chunks.append(cur.strip())
            # if sentence itself bigger than chunk_size, split it raw
            if len(s) > chunk_size:
                # fallback to raw slicing
                start = 0
                while start < len(s):
                    end = start + chunk_size
                    chunks.append(s[start:end].strip())
                    start = end - overlap
                cur = ""
            else:
                cur = s
    if cur:
        chunks.append(cur.strip())
    # add overlap by merging neighbors slightly to preserve context
    if overlap and len(chunks) > 1:
        merged = []
        for i, c in enumerate(chunks):
            if i == 0:
                merged.append(c)
            else:
                prev = merged[-1]
                # create overlap fragment from end of prev
                overlap_fragment = prev[-overlap:] if len(prev) > overlap else prev
                merged.append((overlap_fragment + " " + c).strip())
        chunks = merged
    return chunks

# ========== OCR HELPERS ==========
def ocr_image_bytes(img_bytes, lang="eng"):
    return pytesseract.image_to_string(Image.open(io.BytesIO(img_bytes)), lang=lang)

def ocr_pages_from_pdf_bytes(pdf_bytes, dpi=200, lang="eng", max_workers=4):
    images = convert_from_bytes(pdf_bytes, dpi=dpi)
    texts = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        results = list(ex.map(lambda im: pytesseract.image_to_string(im, lang=lang), images))
    return results

# ========== LOAD & PREPROCESS DOCUMENTS ==========
def load_documents_from_dropbox(incremental=True):
    dbx = get_dropbox_client()
    response = dbx.files_list_folder(FOLDER_PATH, recursive=True)
    docs = []

    # load existing metadata ids for incremental indexing
    existing_ids = set()
    if incremental and os.path.exists(META_PATH):
        try:
            with open(META_PATH, "rb") as f:
                existing = pickle.load(f)
                for d in existing.get("documents", []):
                    md = d.get("metadata", {})
                    if md.get("bates_id"):
                        existing_ids.add(md["bates_id"])
        except Exception:
            existing_ids = set()

    while True:
        for entry in response.entries:
            if isinstance(entry, dropbox.files.FileMetadata) and entry.name.lower().endswith(".pdf"):
                print(f"üìÑ Loading PDF from Dropbox: {entry.name}")
                try:
                    _, res = dbx.files_download(entry.path_lower)
                    # pdf_data = io.BytesIO(res.content)
                    # pdf = fitz.open(stream=pdf_data, filetype="pdf")    
                    pdf_bytes = res.content
                    pdf_stream = fitz.open(stream=io.BytesIO(pdf_bytes), filetype="pdf")

                    # Quick check: does any page have text? If so, don't OCR entire file.
                    has_text_layer = False
                    for p in pdf_stream:
                        text = p.get_text("text").strip()
                        if text:
                            has_text_layer = True
                            break

                    # iterate pages
                    for page_num, page in enumerate(pdf_stream, start=1):
                        text = ""
                        try:
                            if has_text_layer:
                                text = page.get_text("text")
                                if not text or not text.strip():
                                    # fallback to OCR for that page
                                    pix = page.get_pixmap(dpi=200)
                                    img_bytes = pix.tobytes("png")
                                    text = ocr_image_bytes(img_bytes)
                            else:
                                # use pdf2image for batch OCR is more efficient but here we fallback per page
                                pix = page.get_pixmap(dpi=200)
                                img_bytes = pix.tobytes("png")
                                text = ocr_image_bytes(img_bytes)
                        except Exception as e:
                            print(f"Error extracting page {page_num} from {entry.name}: {e}")
                            continue

                        text = clean_page_text(text)
                        if not text:
                            continue

                        chunks = smart_chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)
                        for i, chunk in enumerate(chunks):
                            bates_id = f"{entry.name.replace('.pdf','').upper()}_{page_num:03d}_{i:02d}"
                            if bates_id in existing_ids:
                                # skip already indexed chunk
                                continue
                            docs.append({
                                "id": bates_id,
                                "content": chunk,
                                "metadata": {
                                    "source": os.path.basename(entry.path_display),
                                    "path": entry.path_display,
                                    "page": page_num,
                                    "bates_id": bates_id,
                                    "chunk_index": i,
                                    "chunk_chars": len(chunk),
                                    "has_ocr": not has_text_layer,
                                    "custodian": None,
                                    "collection_id": os.path.basename(FOLDER_PATH)
                                }
                            })
                except Exception as e:
                    print(f"Error reading {entry.name}: {e}")

                #     for page_num, page in enumerate(pdf, start=1):
                #         try:
                #             text = page.get_text("text")
                #             if not text.strip():  # n·∫øu kh√¥ng c√≥ text layer
                #                 raise ValueError("No text layer, using OCR fallback.")
                #         except Exception:
                #             # OCR fallback
                #             img = page.get_pixmap(dpi=200)
                #             img_bytes = img.tobytes("png")
                #             text = pytesseract.image_to_string(Image.open(io.BytesIO(img_bytes)), lang="eng")

                #         text = clean_page_text(text)
                #         if not text.strip():
                #             continue

                #         chunks = split_text(text, CHUNK_SIZE, CHUNK_OVERLAP)

                #         for i, chunk in enumerate(chunks):
                #             # G√°n Bates ID v√† metadata
                #             bates_id = f"{entry.name.replace('.pdf','').upper()}_{page_num:03d}_{i:02d}"
                #             docs.append({
                #                 "id": bates_id,
                #                 "content": chunk,
                #                 "metadata": {
                #                     "source": os.path.basename(entry.path_display),
                #                     "path": entry.path_display,
                #                     "page": page_num,
                #                     "bates_id": bates_id,
                #                     "chunk_index": i,
                #                     "custodian": None,
                #                     "collection_id": os.path.basename(FOLDER_PATH)
                #                 }
                #             })
                # except Exception as e:
                #     print(f"‚ö†Ô∏è Error reading {entry.name}: {e}")

        if not response.has_more:
            break
        response = dbx.files_list_folder_continue(response.cursor)

    print(f"Loaded {len(docs)} new chunks (unique files: {len(set(d['metadata']['source'] for d in docs))}).")
    return docs

def build_faiss_index():
    docs = load_documents_from_dropbox()
    if not docs:
        print("‚ùå No PDF files found in the Dropbox folder.")
        return

    texts = [doc["content"] for doc in docs]
    metadatas = [doc["metadata"] for doc in docs]

    model = load_embedding_model()
    # choose batch size by hardware
    batch_size = 32 if torch.cuda.is_available() else 8

    print(f"üß† Encoding {len(texts)} chunks with batch_size={batch_size} ...")
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        normalize_embeddings=True,
        show_progress_bar=True
    )
    import numpy as np
    embeddings = np.array(embeddings).astype("float32")

    # If index exists, load and append; else create new IP (inner product) index (works with normalized embeddings as cosine)
    os.makedirs(FAISS_DIR, exist_ok=True)
    if os.path.exists(INDEX_PATH):
        try:
            index = faiss.read_index(INDEX_PATH)
            print("Loaded existing FAISS index, appending vectors...")
            index.add(embeddings)
        except Exception as e:
            print(f"Failed to load existing index: {e}. Creating new index.")
            print("üíæ Building FAISS index...")
            index = faiss.IndexFlatIP(embeddings.shape[1])
            index.add(embeddings)
    else:
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)

    faiss.write_index(index, INDEX_PATH)
    print(f"FAISS index saved to {INDEX_PATH}")

    # --- L∆∞u metadata v√†o SQLite ---
    init_sqlite()
    new_docs = [{"content": t, "metadata": m} for t, m in zip(texts, metadatas)]
    insert_metadata(new_docs)

    print(f"‚úÖ Indexed {len(new_docs)} chunks from {len(set(d['metadata']['source'] for d in docs))} PDFs.")
    print(f"üìÅ SQLite metadata: {SQLITE_DB_PATH}")
    
    # # merge metadata with existing metadata file (incremental)
    # existing_docs = []
    # if os.path.exists(META_PATH):
    #     try:
    #         with open(META_PATH, "rb") as f:
    #             existing_docs = pickle.load(f).get("documents", [])
    #     except Exception:
    #         existing_docs = []

    # new_docs = [{"page_content": t, "metadata": m} for t, m in zip(texts, metadatas)]
    # merged = {"documents": existing_docs + new_docs}
    # with open(META_PATH, "wb") as f:
    #     pickle.dump(merged, f)

    # print("‚úÖ Indexed:")
    # print(f"   - {len(new_docs)} chunks total")
    # print(f"   - {len(set(d['metadata']['source'] for d in docs))} PDF files")
    # print(f"   - Total metadata entries: {len(merged['documents'])}")
    # print(f"üìÅ Index saved to: {INDEX_PATH}")
    # print(f"üìÅ Metadata saved to: {META_PATH}")



